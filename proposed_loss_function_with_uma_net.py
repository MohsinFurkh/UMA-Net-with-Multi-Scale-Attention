# -*- coding: utf-8 -*-
"""Proposed Loss Function with UMA-Net

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/mohsinfurkh/proposed-loss-function-with-uma-net.b66c3104-1dc3-4a5c-9021-55397018383f.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250604/auto/storage/goog4_request%26X-Goog-Date%3D20250604T065253Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D139532bb01132ce14cff7f2410f5b739cc7833a671f42ede3c2ed5562e9ac88ce9d9d28c55fff26931e449fb90bb873e87744d82b1de947b3d6492083c6d21a89457fdf1f39b03ec915176ad6e67e3d51a7220c95e751ba04db2d946c826ab2982118701c18b281c6be931b9e75b19c412369ca4d6ae646a1eabe9f09106bf18e36ae07a80360215bf75b08531de815adc3f3719bc087e3e6a586b0cd7695b197db681f7bd2598a92bb5c71c1b6113ddceaa73214d5595ac5cee9aaa483eecc3dddaf025ffad402b10e9ef525fff3c05d50d4c15b688191f82b8de8e77d8a50ce9e02a1f791763cf8c05a6f024be4eaf3b7223f94a66e6a875ed2f3b5580e1de
"""

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import tensorflow.keras.backend as K
from scipy.stats import ttest_rel

def dice_coefficient(y_true, y_pred, smooth=1):
    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)
    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)

# Function to load and preprocess images
def load_images(image_dir, mask_dir, img_height, img_width):
    images = []
    masks = []

    image_filenames = os.listdir(image_dir)
    for image_filename in image_filenames:
        # Load and preprocess the image
        image_path = os.path.join(image_dir, image_filename)
        image = load_img(image_path, target_size=(img_height, img_width))
        image = img_to_array(image) / 255.0  # Normalize image
        images.append(image)

        # Load and preprocess the mask
        mask_path = os.path.join(mask_dir, image_filename)
        mask = load_img(mask_path, target_size=(img_height, img_width))
        mask = img_to_array(mask) / 255.0  # Normalize mask
        masks.append(mask)

    return np.array(images), np.array(masks)

def compute_class_statistics(images, masks):
    class_means = {}
    class_vars = {}

    for class_value in np.unique(masks):
        class_pixels = images[masks == class_value]
        class_means[class_value] = np.mean(class_pixels)
        class_vars[class_value] = np.var(class_pixels)

    # Convert to arrays for use in the loss function
    class_means = np.array([class_means.get(0, 0), class_means.get(1, 1)])
    class_vars = np.array([class_vars.get(0, 0), class_vars.get(1, 1)])

    return class_means, class_vars

def dice_coef(y_true, y_pred):
    intersection = np.sum(y_true * y_pred)
    union = np.sum(y_true) + np.sum(y_pred)
    return (2.0 * intersection) / (union + 1e-7)

def iou_coef(y_true, y_pred):
    intersection = np.sum(y_true * y_pred)
    union = np.sum(y_true) + np.sum(y_pred) - intersection
    return (intersection + 1e-7) / (union + 1e-7)

# Function to calculate metrics
def calculate_metrics(y_true, y_pred):
    # Ensure the shapes are the same
    if y_true.shape != y_pred.shape:
        raise ValueError(f'Shape mismatch: y_true shape {y_true.shape} and y_pred shape {y_pred.shape}')

    y_true_flat = y_true.flatten()
    y_pred_flat = y_pred.flatten()

    # Threshold predictions to get binary output
    y_pred_flat = (y_pred_flat > 0.5).astype(int)
    y_true_flat = (y_true_flat > 0.5).astype(int)

    # Calculate metrics
    tp = np.sum((y_true_flat == 1) & (y_pred_flat == 1))
    tn = np.sum((y_true_flat == 0) & (y_pred_flat == 0))
    fp = np.sum((y_true_flat == 0) & (y_pred_flat == 1))
    fn = np.sum((y_true_flat == 1) & (y_pred_flat == 0))

    dice = 2 * tp / (2 * tp + fp + fn)
    iou = tp / (tp + fp + fn)
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    sensitivity = tp / (tp + fn)
    specificity = tn / (tn + fp)
    print("\n\n *********Results on Test Set********* \n\n")
    print(f'Dice coefficient: {dice:.4f}')
    print(f'IoU: {iou:.4f}')
    print(f'Accuracy: {accuracy:.4f}')
    print(f'Sensitivity: {sensitivity:.4f}')
    print(f'Specificity: {specificity:.4f}')

    # Calculate Dice scores for each test image
    dice_scores = [dice_coef(y_true[i], y_pred[i]) for i in range(len(y_true))]
    # Calculate IoU scores for each test image
    IoU_scores = [iou_coef(y_true[i], y_pred[i]) for i in range(len(y_true))]

    return dice_scores, IoU_scores

def plot_all_prediction_masks(X_test, y_test, results_dict, fold):
    num_samples = min(20, X_test.shape[0])  # Plot up to 10 samples
    num_loss_functions = len(results_dict)

    # Shuffle indices to select random samples
    indices = np.random.choice(X_test.shape[0], num_samples, replace=False)

    for i, idx in enumerate(indices):
        plt.figure(figsize=(5 * (num_loss_functions + 1), 5))
        # Adjust spacing and margins
        plt.subplots_adjust(left=0.0, right=0.01, top=0.01, bottom=0.0, wspace=0.0, hspace=0.0)

        original_image = cv2.cvtColor(X_test[idx], cv2.COLOR_BGR2RGB)
        ground_truth_mask = y_test[idx]
        if ground_truth_mask.max() > 1:
            ground_truth_mask = ((ground_truth_mask - ground_truth_mask.min()) / (ground_truth_mask.max() - ground_truth_mask.min()) * 255).astype(np.uint8)
        else:
            ground_truth_mask = (ground_truth_mask * 255).astype(np.uint8)

        # Apply Canny edge detector on ground truth mask
        ground_truth_edges = cv2.Canny(ground_truth_mask.astype(np.uint8), 100, 200)

        # Create a copy of the original image to draw boundaries
        img_with_gt_boundary = original_image.copy()
        img_with_gt_boundary[ground_truth_edges != 0] = [255, 0, 0]  # Red color for ground truth boundaries



        for j, (loss_function_name, result) in enumerate(results_dict.items()):
            predicted_masks = result['predicted_masks']
            dice_scores = result['dice_scores']
            IoU_scores = result['IoU_scores']

            predicted_mask = predicted_masks[idx]

            # Normalize predicted mask to 0-255 if necessary
            if predicted_mask.max() > 1:
                predicted_mask = ((predicted_mask - predicted_mask.min()) / (predicted_mask.max() - predicted_mask.min()) * 255).astype(np.uint8)
            else:
                predicted_mask = (predicted_mask * 255).astype(np.uint8)

            # Apply Canny edge detector on predicted mask
            predicted_edges = cv2.Canny(predicted_mask.astype(np.uint8), 50, 150)

            # Create a copy of the original image to draw boundaries
            img_with_pred_boundary = original_image.copy()
            img_with_pred_boundary[predicted_edges != 0] = [0, 255, 0]  # Green color for predicted boundaries

            # Combine both ground truth and predicted boundaries
            combined_img = img_with_gt_boundary.copy()
            combined_img[predicted_edges != 0] = [0, 255, 0]  # Green color for predicted boundaries

            plt.subplot(1, num_loss_functions + 1, j + 2)
            plt.imshow(combined_img)
            plt.title(f'{loss_function_name}\nDice: {dice_scores[idx]:.4f}\nIoU: {IoU_scores[idx]:.4f}', fontsize=20)
            plt.axis('off')

        plt.tight_layout()
        plt.savefig(f'sample_{i + 1}_{fold+1}_predictions_with_boundaries.png', dpi=300)
        plt.show()

# Perform paired t-tests

def perform_paired_t_tests(results_dict):
    # Flatten the proposed dice scores
    proposed_dice_scores = np.array(results_dict['Proposed Loss']['dice_scores']).flatten()
    p_values = {}

    # Debugging: Print proposed dice scores
    # print("Proposed Loss Dice Scores:", proposed_dice_scores)

    for name, values in results_dict.items():
        if name != 'Proposed Loss':
            # Flatten the dice scores
            dice_scores = np.array(values['dice_scores']).flatten()

            # Debugging: Print current comparison dice scores
            # print(f"\n{name} Dice Scores:", dice_scores)

            # Check for identical values
            if np.array_equal(proposed_dice_scores, dice_scores):
                print(f"Identical Dice scores for Proposed Loss and {name}, skipping t-test.")
                p_values[name] = np.nan
                continue

            # Check for valid length
            if len(proposed_dice_scores) <= 1 or len(dice_scores) <= 1:
                print(f"Insufficient data for Proposed Loss and {name}, skipping t-test.")
                p_values[name] = np.nan
                continue

            # Check for variance
            differences = proposed_dice_scores - dice_scores
            if np.var(differences) == 0:
                print(f"No variance in differences between Proposed Loss and {name}, skipping t-test.")
                p_values[name] = np.nan
                continue

            # Perform t-test
            t_stat, p_value = ttest_rel(proposed_dice_scores, dice_scores)
            p_values[name] = p_value

    # Print results in table format
    print("\nComparison                        p-value")
    print("------------------------------------------")
    for name, p_value in p_values.items():
        print(f"Proposed Loss Vs {name:25}  {p_value:.6f}")

import pandas as pd
import seaborn as sns

def plot_violin(results_dict, fold):
    data = []

    for loss_function, metrics in results_dict.items():
        for dice_score in metrics['dice_scores']:
            dsc_score = np.array(dice_score).flatten()
            data.extend([{'Loss Function': loss_function, 'Score Type': 'Dice', 'Score Value': val} for val in dsc_score])
        for iou_score in metrics['iou_scores']:
            iou_ = np.array(iou_score).flatten()
            data.extend([{'Loss Function': loss_function, 'Score Type': 'IoU', 'Score Value': val} for val in iou_])

    df = pd.DataFrame(data)


    # Plot violin plots
    plt.figure(figsize=(12, 6))
    sns.violinplot(x='Loss Function', y='Score Value', hue='Score Type', data=df, split=True, inner="quart")
    plt.title('Distribution of Dice and IoU Scores for Different Loss Functions')
    plt.xlabel('Loss Function')
    plt.ylabel('Score Value')
    plt.legend(title='Score Type')
    plt.legend(title='Score Type')
    plt.savefig(f'plot_{fold+1}_violin.png', dpi=300)
    plt.show()

def plot_history(results, fold):
    plt.figure(figsize=(12, 6))
    for loss_function_name, result in results.items():
        #plt.plot(result['history']['loss'], label=f'{loss_function_name} Training Loss')
        plt.plot(result['history']['val_loss'], label=f'{loss_function_name} Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    plt.figure(figsize=(12, 6))
    for loss_function_name, result in results.items():
        #plt.plot(result['history']['dice_coefficient'], label=f'{loss_function_name} Training Dice Coefficient')
        plt.plot(result['history']['val_dice_coefficient'], label=f'{loss_function_name} Validation Dice Coefficient')
    plt.title('Training and Validation Dice Coefficient')
    plt.xlabel('Epochs')
    plt.ylabel('Dice Coefficient')
    plt.legend()
    plt.savefig(f'plot_{fold+1}_history.png', dpi=300)
    plt.show()

from tensorflow.keras.losses import BinaryCrossentropy
def custom_binary_crossentropy_loss(y_true, y_pred):
    # Convert y_true to single-channel grayscale
    y_true_single_channel = tf.image.rgb_to_grayscale(y_true)

    # Use BinaryCrossentropy loss
    loss_fn = tf.keras.losses.BinaryCrossentropy()
    return loss_fn(y_true_single_channel, y_pred)

import tensorflow as tf
import tensorflow.keras.backend as K

###############################################################################

def euclidean_distance(x, y):
    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))

def hausdorff_distance(y_true, y_pred):
    d1 = K.mean(K.max(euclidean_distance(y_true, y_pred), axis=1))
    d2 = K.mean(K.max(euclidean_distance(y_pred, y_true), axis=1))
    return 0.5 * (d1 + d2)

def hausdorff_distance_loss(y_true, y_pred):
    return hausdorff_distance(y_true, y_pred)

###############################################################################

# Define the Tversky loss function
def tverskyloss(alpha=0.7, beta=0.3):
    def tversky_loss(y_true, y_pred):
        numerator = tf.reduce_sum(y_true * y_pred, axis=-1)
        denominator = numerator + alpha * tf.reduce_sum(y_true * (1 - y_pred), axis=-1) + beta * tf.reduce_sum((1 - y_true) * y_pred, axis=-1)
        return 1 - (numerator + 1) / (denominator + 1)
    return tversky_loss
# Initialize the loss functions
tl_loss = tverskyloss()
###############################################################################

def dice_loss(y_true, y_pred):
    numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=-1)
    denominator = tf.reduce_sum(y_true + y_pred, axis=-1)
    return 1 - (numerator + 1e-7) / (denominator + 1e-7)

###############################################################################

# Define the ensemble loss class
class EnsembleLoss:
    def __init__(self, loss_fns, weights=None):
        self.loss_fns = loss_fns
        if weights is None:
            self.weights = [1.0] * len(loss_fns)
        else:
            self.weights = weights

    def __call__(self, y_true, y_pred):
        total_loss = 0
        for weight, loss_fn in zip(self.weights, self.loss_fns):
            total_loss += weight * loss_fn(y_true, y_pred)
        return total_loss

# Callback to dynamically adjust weights
class DynamicWeightingCallback(tf.keras.callbacks.Callback):
    def __init__(self, ensemble_loss, validation_data):
        self.ensemble_loss = ensemble_loss
        self.validation_data = validation_data

    def on_epoch_end(self, epoch, logs=None):
        val_losses = []
        for loss_fn in self.ensemble_loss.loss_fns:
            val_loss = tf.reduce_mean(loss_fn(self.validation_data[0], self.validation_data[1])).numpy()
            val_losses.append(val_loss)

        total_val_loss = sum(val_losses)
        self.ensemble_loss.weights = [1 - (val_loss / total_val_loss) for val_loss in val_losses]
        print(f"Updated weights: {self.ensemble_loss.weights}")

input_shape = (128,128,3)
rows = 128
cols = 128

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from scipy.spatial.distance import directed_hausdorff
from keras.callbacks import ModelCheckpoint
from keras import layers, models
import tensorflow.keras.backend as K
from tensorflow.keras.optimizers import Adam

def conv_block(x, filter_size, size, dropout, batch_norm=False):

    conv = layers.Conv2D(size, (filter_size, filter_size), padding="same")(x)
    if batch_norm is True:
        conv = layers.BatchNormalization(axis=3)(conv)
    conv = layers.Activation("relu")(conv)

    conv = layers.Conv2D(size, (filter_size, filter_size), padding="same")(conv)
    if batch_norm is True:
        conv = layers.BatchNormalization(axis=3)(conv)
    conv = layers.Activation("relu")(conv)

    if dropout > 0:
        conv = layers.Dropout(dropout)(conv)

    return conv
def repeat_elem(tensor, rep):
    # lambda function to repeat Repeats the elements of a tensor along an axis
    #by a factor of rep.
    # If tensor has shape (None, 256,256,3), lambda will return a tensor of shape
    #(None, 256,256,6), if specified axis=3 and rep=2.

     return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),
                          arguments={'repnum': rep})(tensor)
def res_conv_block(x, filter_size, size, dropout, batch_norm=False):
    '''
    Residual convolutional layer.
    Two variants....
    Either put activation function before the addition with shortcut
    or after the addition (which would be as proposed in the original resNet).

    1. conv - BN - Activation - conv - BN - Activation
                                          - shortcut  - BN - shortcut+BN

    2. conv - BN - Activation - conv - BN
                                     - shortcut  - BN - shortcut+BN - Activation

    Check fig 4 in https://arxiv.org/ftp/arxiv/papers/1802/1802.06955.pdf
    '''

    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(x)
    if batch_norm is True:
        conv = layers.BatchNormalization(axis=3)(conv)
    conv = layers.Activation('relu')(conv)

    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(conv)
    if batch_norm is True:
        conv = layers.BatchNormalization(axis=3)(conv)
    #conv = layers.Activation('relu')(conv)    #Activation before addition with shortcut
    if dropout > 0:
        conv = layers.Dropout(dropout)(conv)

    shortcut = layers.Conv2D(size, kernel_size=(1, 1), padding='same')(x)
    if batch_norm is True:
        shortcut = layers.BatchNormalization(axis=3)(shortcut)

    res_path = layers.add([shortcut, conv])
    res_path = layers.Activation('relu')(res_path)    #Activation after addition with shortcut (Original residual block)
    return res_path
def gating_signal(input, out_size, batch_norm=False):
    """
    resize the down layer feature map into the same dimension as the up layer feature map
    using 1x1 conv
    :return: the gating feature map with the same dimension of the up layer feature map
    """
    x = layers.Conv2D(out_size, (1, 1), padding='same')(input)
    if batch_norm:
        x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    return x
def attention_block(x, gating, inter_shape):
    shape_x = K.int_shape(x)
    shape_g = K.int_shape(gating)

    # Getting the x signal to the same shape as the gating signal
    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same')(x)
    shape_theta_x = K.int_shape(theta_x)

    # Getting the gating signal to the same number of filters as the inter_shape
    phi_g = layers.Conv2D(inter_shape, (1, 1), padding='same')(gating)

    # Calculate the strides, ensuring they are greater than zero
    stride_x = max(shape_theta_x[1] // shape_g[1], 1)
    stride_y = max(shape_theta_x[2] // shape_g[2], 1)

    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3),
                                        strides=(stride_x, stride_y),
                                        padding='same')(phi_g)

    # Calculate the upsampling size, ensuring it's valid
    up_size_x = max(shape_theta_x[1] // K.int_shape(upsample_g)[1], 1)
    up_size_y = max(shape_theta_x[2] // K.int_shape(upsample_g)[2], 1)

    if (up_size_x > 1) or (up_size_y > 1):
        upsample_g = layers.UpSampling2D(size=(up_size_x, up_size_y))(upsample_g)

    concat_xg = layers.add([upsample_g, theta_x])
    act_xg = layers.Activation('relu')(concat_xg)
    psi = layers.Conv2D(1, (1, 1), padding='same')(act_xg)
    sigmoid_xg = layers.Activation('sigmoid')(psi)
    shape_sigmoid = K.int_shape(sigmoid_xg)
    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)

    upsample_psi = layers.Lambda(lambda x: K.repeat_elements(x, shape_x[3], axis=-1))(upsample_psi)

    y = layers.multiply([upsample_psi, x])

    result = layers.Conv2D(shape_x[3], (1, 1), padding='same')(y)
    result_bn = layers.BatchNormalization()(result)
    return result_bn

from tensorflow.keras.layers import Activation, BatchNormalization, Conv2D, AveragePooling2D, UpSampling2D

def ASPP(x, filter):
    shape = x.shape

    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(x)
    y1 = Conv2D(filter, 1, padding="same")(y1)
    y1 = BatchNormalization()(y1)
    y1 = Activation("relu")(y1)
    y1 = UpSampling2D((shape[1], shape[2]), interpolation='bilinear')(y1)

    y2 = Conv2D(filter, 1, dilation_rate=1, padding="same", use_bias=False)(x)
    y2 = BatchNormalization()(y2)
    y2 = Activation("relu")(y2)

    y3 = Conv2D(filter, 3, dilation_rate=6, padding="same", use_bias=False)(x)
    y3 = BatchNormalization()(y3)
    y3 = Activation("relu")(y3)

    y4 = Conv2D(filter, 3, dilation_rate=12, padding="same", use_bias=False)(x)
    y4 = BatchNormalization()(y4)
    y4 = Activation("relu")(y4)

    y5 = Conv2D(filter, 3, dilation_rate=18, padding="same", use_bias=False)(x)
    y5 = BatchNormalization()(y5)
    y5 = Activation("relu")(y5)

    y = layers.Concatenate()([y1, y2, y3, y4, y5])

    y = Conv2D(filter, 1, dilation_rate=1, padding="same", use_bias=False)(y)
    y = BatchNormalization()(y)
    y = Activation("relu")(y)

    return y

def UMA_Net(input_shape, NUM_CLASSES=1, dropout_rate=0.0, batch_norm=True):
    # Load pre-trained MobileNetV1 without the top fully connected layer
    base_model = tf.keras.applications.MobileNet(input_shape=input_shape, include_top=False, weights='imagenet')

    # Extract feature maps from MobileNet at different stages
    conv_128 = base_model.get_layer('conv_pw_1_relu').output  # Corresponds to early features
    conv_64 = base_model.get_layer('conv_pw_3_relu').output   # Intermediate feature map
    conv_32 = base_model.get_layer('conv_pw_5_relu').output   # Deeper feature map
    conv_16 = base_model.get_layer('conv_pw_11_relu').output  # Even deeper feature map
    conv_8 = base_model.get_layer('conv_pw_13_relu').output   # Bottleneck, deepest feature ma

    # Bottleneck ASPP layer
    bottleneck = ASPP(conv_8, 512)

    # Upsampling layers
    # UpRes 6
    gating_16 = gating_signal(bottleneck, 512, batch_norm)
    att_16 = attention_block(conv_16, gating_16, 512)
    up_16 = layers.UpSampling2D(size=(2, 2))(bottleneck)
    up_16 = layers.concatenate([up_16, att_16], axis=-1)
    up_conv_16 = res_conv_block(up_16, 3, 512, dropout_rate, batch_norm)

    # UpRes 7
    gating_32 = gating_signal(up_conv_16, 256, batch_norm)
    att_32 = attention_block(conv_32, gating_32, 256)
    up_32 = layers.UpSampling2D(size=(2, 2))(up_conv_16)
    up_32 = layers.concatenate([up_32, att_32], axis=-1)
    up_conv_32 = res_conv_block(up_32, 3, 256, dropout_rate, batch_norm)

    # UpRes 8
    gating_64 = gating_signal(up_conv_32, 128, batch_norm)
    att_64 = attention_block(conv_64, gating_64, 128)
    up_64 = layers.UpSampling2D(size=(2, 2))(up_conv_32)
    up_64 = layers.concatenate([up_64, att_64], axis=-1)
    up_conv_64 = res_conv_block(up_64, 3, 128, dropout_rate, batch_norm)

    # UpRes 9
    gating_128 = gating_signal(up_conv_64, 64, batch_norm)
    att_128 = attention_block(conv_128, gating_128, 64)
    up_128 = layers.UpSampling2D(size=(2, 2))(up_conv_64)
    up_128 = layers.concatenate([up_128, att_128], axis=-1)
    up_conv_128 = res_conv_block(up_128, 3, 64, dropout_rate, batch_norm)

    # Final convolution layer
    conv_final = layers.Conv2D(NUM_CLASSES, (1, 1))(up_conv_128)
    conv_final = layers.BatchNormalization(axis=-1)(conv_final)
    conv_final = layers.Activation('sigmoid')(conv_final)

    # Optional: Add a final upsampling layer to match the input size
    conv_final = layers.UpSampling2D(size=(2, 2))(conv_final)  # Upsample to match the input size

    # Model integration
    model = models.Model(inputs=base_model.input, outputs=conv_final, name="UMA-Net")

    return model

model = UMA_Net(input_shape)

model.save("UMA_Net.h5")

from sklearn.model_selection import train_test_split

# Step 1: Loading and Preprocessing the Images

ground_truth_dir_path = "/kaggle/input/busi-segmentation/masks/"
original_image_dir_path = "/kaggle/input/busi-segmentation/images/"

# Load images and masks
images, masks = load_images(original_image_dir_path, ground_truth_dir_path, rows, cols)

# Compute class statistics from original images
class_means, class_vars = compute_class_statistics(images, masks)

print("Class Means:", class_means)
print("Class Variances:", class_vars)

print(np.shape(images))
print(np.shape(masks))

loss_funs = [custom_binary_crossentropy_loss, dice_loss, hausdorff_distance_loss, tl_loss]
weights = [0.25, 0.25, 0.25, 0.25]  # Initial equal weighting

# Create the ensemble loss
ensemble_loss = EnsembleLoss(loss_funs, weights)

# Define different versions of the loss function
loss_functions = {
    "BCE Loss": custom_binary_crossentropy_loss,
    'Dice Loss': dice_loss,
    'Hausdorff Loss': hausdorff_distance_loss,
    'Tversky Loss': tl_loss,
    "Proposed Loss": ensemble_loss
}

def train_model(model, train_generator, X_val, y_val, steps_per_epoch, epochs):
    start_time = time.time()
    history = model.fit(train_generator,
                        steps_per_epoch= steps_per_epoch,
                        validation_data=(X_val, y_val),
                        epochs=epochs,
                        callbacks=[reduce_lr],
                       verbose=0)
    end_time = time.time()
    total_training_time = end_time - start_time
    actual_epochs = len(history.history['loss'])
    mean_training_time_per_epoch = total_training_time / actual_epochs
    return history, mean_training_time_per_epoch

from tensorflow.keras.callbacks import ReduceLROnPlateau
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',    # Metric to monitor
    factor=0.2,            # Factor by which the learning rate will be reduced
    patience=5,           # Number of epochs with no improvement after which learning rate will be reduced
    verbose=0,             # Verbosity mode (0 or 1)
    mode='auto',           # One of {'auto', 'min', 'max'}. In 'min' mode, the learning rate will be reduced when the quantity monitored has stopped decreasing; in 'max' mode it will be reduced when the quantity monitored has stopped increasing.
    min_lr=1e-6            # Lower bound on the learning rate
)

import time
import pickle
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
# Number of folds for cross-validation
n_splits = 3
# Parameters
img_height, img_width = 128, 128  # Example dimensions, adjust as necessary
batch_size = 16  # Adjust based on available memory and computational resources
# Initialize KFold
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Convert images and masks to numpy arrays for indexing
images = np.array(images)
masks = np.array(masks)

# Perform 5-fold cross-validation
for fold, (train_index, test_index) in enumerate(kf.split(images)):
    print(f"\nFold {fold+1}/{n_splits}")
    X_train, X_test = images[train_index], images[test_index]
    y_train, y_test = masks[train_index], masks[test_index]

    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
    # Define the ImageDataGenerator for augmentation
    datagen = ImageDataGenerator(
        rotation_range=20,
        width_shift_range=0.1,
        height_shift_range=0.1,
        shear_range=0.1,
        zoom_range=0.1,
        horizontal_flip=True,
        fill_mode='nearest'
    )

    # Fit the generator on the training data (if necessary)
    datagen.fit(X_train)

    # Use the generator
    train_generator = datagen.flow(X_train, y_train, batch_size=batch_size)
    steps_per_epoch = np.ceil(len(X_train) / batch_size).astype(int)

    # Initialize a dictionary to collect predicted masks, IOU and Dice scores for each loss function
    results_dict = {}
    result_scores = {name: {'dice_scores': [], 'iou_scores': []} for name in loss_functions.keys()}

    for loss_function_name, loss_function in loss_functions.items():
        # Compile the model
        model = UMA_Net(input_shape)
        model.compile(optimizer='adam', loss=loss_function, metrics=[dice_coefficient])

        print(f"\nTraining UMA-Unet model with {loss_function_name} function for fold {fold+1}...\n")

        # Train the model
        history, mean_training_time_per_epoch  = train_model(model, train_generator, X_val, y_val, steps_per_epoch, epochs=100)

        print(f"Mean training time per epoch for {loss_function_name} on fold {fold+1}: {mean_training_time_per_epoch:.2f} seconds")
        # Store the history
        results_dict[loss_function_name] = history.history

        # Step 3: Evaluating the Model on the Test Set

        # Evaluate the model on the test set
        print(f"\nResults for {loss_function_name} function on fold {fold+1}:")
        y_pred = model.predict(X_test)
        y_pred = np.repeat(y_pred, 3, axis=-1)
        dice_scores, iou_scores = calculate_metrics(y_test, y_pred)

        result_scores[loss_function_name]['dice_scores'].append(dice_scores)
        result_scores[loss_function_name]['iou_scores'].append(iou_scores)

        results_dict[loss_function_name] = {
            'predicted_masks': y_pred,
            'IoU_scores': iou_scores,
            'dice_scores': dice_scores,
            'history': history.history
            }
    # Call your plotting functions here
    perform_paired_t_tests(result_scores)
    plot_violin(result_scores, fold)
    plot_history(results_dict, fold)
    plot_all_prediction_masks(X_test, y_test, results_dict, fold)
    with open(f'results_data_{fold+1}.pkl', 'wb') as file:
        pickle.dump({'results_dict': results_dict, 'result_scores': result_scores}, file)